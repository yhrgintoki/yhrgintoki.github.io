<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Robot-assited Feeding on Food Manipulation</title>
    <link>https://yhrgintoki.github.io/</link>
    <description>Recent content in Robot-assited Feeding on Food Manipulation</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 03 Nov 2019 16:59:00 +0700</lastBuildDate>
    
	<atom:link href="https://yhrgintoki.github.io/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Wireless Perception Module</title>
      <link>https://yhrgintoki.github.io/hardware/wpm/</link>
      <pubDate>Wed, 28 Nov 2018 15:14:54 +1000</pubDate>
      
      <guid>https://yhrgintoki.github.io/hardware/wpm/</guid>
      <description>A module which interfaces sensors and wirelessly transmits high-bandwidth data over WiFi</description>
    </item>
    
    <item>
      <title>Demos</title>
      <link>https://yhrgintoki.github.io/demos/</link>
      <pubDate>Sun, 03 Nov 2019 16:59:00 +0700</pubDate>
      
      <guid>https://yhrgintoki.github.io/demos/</guid>
      <description> Autonomous robot feeding for upper-extremity mobility impaired people: Integrating sensing, perception, learning, motion planning, and robot control.
T. Bhattacharjee, D. Gallenberger, D. Dubois, L. L&amp;rsquo;Écuyer-Lapiere, Y. Kim, A. Mandalika, R. Scalise, R. Qu, H. Song, E. Gordon, and S.S. Srinivasa.
Conference on Neural Information Processing Systems, 2018
Best Demo Award Winner
  
Outreach  TTI/Vanguard Visit and Demo  March 2, 2020 UW: Robotics Lab &amp;nbsp;  American Association for the Advancement of Science (AAAS) Demo  February 14-15, 2020 Washington Convention Center &amp;nbsp;  Computing Open House 2019  December 7, 2019 UW: Gates Center &amp;nbsp;  Clive Thompson Video Shoot  October 23, 2019 UW: Robotics Lab Photography by Ariana McLaughlin &amp;nbsp;  Toyota Research Institute Demo  October 2, 2019 UW: Robotics Lab &amp;nbsp;  US Patent Office Demo  July 16, 2019 UW: Wallace Hall, AMP Lab &amp;nbsp;  Engineering Discover Days 2019  April 26, 2019 UW: Robotics Lab &amp;nbsp;  Puget Sound Business Journal Demo  April 25, 2019; Published May 3, 2019 UW: Robotics Lab &amp;nbsp;  NeurIPS 2018 Demonstration  December 5, 2018 &amp;nbsp;  Computing Open House 2018  December 2, 2018 UW: Allen Center &amp;nbsp;  CSE Woman’s Day Demo  April 6, 2018 UW: Robotics Lab       </description>
    </item>
    
    <item>
      <title>Open Datasets</title>
      <link>https://yhrgintoki.github.io/datasets/</link>
      <pubDate>Sun, 03 Nov 2019 16:59:00 +0700</pubDate>
      
      <guid>https://yhrgintoki.github.io/datasets/</guid>
      <description>A Dataset of Food Manipulation Strategies.
A dataset of multimodal sensing modalities (forces, torques, poses, RGBD images) for feeding task.
A Dataset of Food Items with Skewering Location and Rotation Masks.
A dataset of annotations of skewerable food items for image-based bite acquisition and bite transfer task.
A Dataset of Robot Bite Acquisition Trials on Solid Food Using Different Manipulation Strategies.
A dataset of attempts by the Autonomous Dextrous Arm (ADA), a JACO 2 arm equipped with an eye-in-hand camera and a force-torque sensor, to skewer one of 16-types of food given one of 6 manipulation strategies (3 pitch angles X 2 roll angles).</description>
    </item>
    
    <item>
      <title>Publications</title>
      <link>https://yhrgintoki.github.io/publications/</link>
      <pubDate>Sun, 03 Nov 2019 16:59:00 +0700</pubDate>
      
      <guid>https://yhrgintoki.github.io/publications/</guid>
      <description>Is More Autonomy Always Better? Exploring Preferences of Users with Mobility Impairments in Robot-assisted Feeding.
T. Bhattacharjee, E.K. Gordon, R. Scalise, M.E. Cabrera, A. Caspi, M. Cakmak, and S.S. Srinivasa.
In ACM/IEEE International Conference on Human-Robot Interaction, 2020.
Towards Robotic Feeding: Role of Haptics in Fork-based Food Manipulation.
T. Bhattacharjee, G. Lee, H. Song, and S.S. Srinivasa.
IEEE Robotics and Automation Letters, 2019.
Transfer depends on Acquisition: Analyzing Manipulation Strategies for Robotic Feeding.</description>
    </item>
    
    <item>
      <title>Videos</title>
      <link>https://yhrgintoki.github.io/videos/</link>
      <pubDate>Sun, 03 Nov 2019 16:59:00 +0700</pubDate>
      
      <guid>https://yhrgintoki.github.io/videos/</guid>
      <description>Evaluating an Assistive-Feeding Robot with Users with Mobility Limitations In this work, we explore user preferences for different modes of autonomy for robot-assisted feeding given perceived error risks and also analyze the effect of input modalities on technology acceptance.
Specifically, we tested: Speed(Fast vs. Slow), Interface (Web-based vs. Voice-based), Environment (Social vs. Individual), Level of Autonomy: (Full vs. Partial vs. Low)
 
Online Learning for Food Manipulation This video shows how a robot can learn to skewer previously-unseen food items with different action distributions using online learning with a contextual bandit formulation.</description>
    </item>
    
    <item>
      <title>Press</title>
      <link>https://yhrgintoki.github.io/press/</link>
      <pubDate>Thu, 22 Feb 2018 17:01:34 +0700</pubDate>
      
      <guid>https://yhrgintoki.github.io/press/</guid>
      <description>This is press.</description>
    </item>
    
    <item>
      <title>Robot-assited Feeding</title>
      <link>https://yhrgintoki.github.io/overview/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://yhrgintoki.github.io/overview/</guid>
      <description>By Tapomayukh Bhattacharjee
 Eating free-form food is one of the most intricate manipulation tasks we perform in our daily lives, demanding robust nonprehensile manipulation of a deformable hard-to-model target. Automating food manipulation is daunting as the universe of foods, cutlery, and human strategies is massive. To understand how humans manipulate food items during feeding and to explore ways to adapt their strategies to robots, we collected human trajectories by asking them to pick up food and feed it to a mannequin.</description>
    </item>
    
    <item>
      <title>Tactile Sensor</title>
      <link>https://yhrgintoki.github.io/hardware/gts/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://yhrgintoki.github.io/hardware/gts/</guid>
      <description>Introduction
Vision-based tactile sensors that employ cameras to capture high-resolution tactile information ofa soft elastomer pad area. These types of sensors [1]–[5] have seen a rise in popularity and development as they have demonstrated utility in manipulation, identification, and inspection tasks. The idea is inspired by the research in Ted Adelson&amp;rsquo;s group.
Measurement of contact forces of a robot gripper plays an important role in manipulation tasks, specifically food manipulation where forces on a utensil may convey food properties or states.</description>
    </item>
    
  </channel>
</rss>